{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to $\\texttt{flarestack}$\n",
    "### Links ###\n",
    "* Slides:  https://events.icecube.wisc.edu/event/140/contributions/7689/attachments/6114/7347/IceCube_tech_workshop_flarestack.pdf\n",
    "* GitHub: https://github.com/icecube/flarestack\n",
    "* More examples: https://mybinder.org/v2/gh/icecube/flarestack/master\n",
    "\n",
    "### Logging ###\n",
    "$\\texttt{flarestack}$ uses logging. So to get an idea of what is going on, let's turn it on!  \\\n",
    "(We will use the INFO level. If you run into a problem and really want to know what's going on use DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level='INFO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/thomasahrens/Desktop/IceCube/flarestack/scratch'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['FLARESTACK_SCRATCH_DIR']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation ##\n",
    "Execute\n",
    "```bash\n",
    "pip install --prefix=/path/to/lib flarestack\n",
    "```\n",
    "to install $\\texttt{flarestack}$ into `/path/to/lib` space ***prior to executing this notebook***.\n",
    "This will also install all necessary dependencies.  \n",
    "For the purpose of this workshop: `path/to/lib=/scratch/<your_username>/`\n",
    "\n",
    "### Set up external directories ###\n",
    "\n",
    "To specify your **dataset directory** do\n",
    "```bash\n",
    "export FLARESTACK_DATASET_DIR=/path/to/datasets\n",
    "```\n",
    "If you are running at DESY or WIPAC this is not necesarry.  \n",
    "The code products such as data, plots, caches, etc will be stored in a separate scratch directory.\n",
    "To specify run\n",
    "```bash\n",
    "export FLARESTACK_SCRATCH_DIR=/path/to/scratch\n",
    "```\n",
    "If not given, defaults to home directory. For now we can use `/scratch/<your_username>/flarestack_scratch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:flarestack.shared:Scratch Directory is: /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/\n",
      "INFO:flarestack.shared:Found Directory: /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/\n",
      "INFO:flarestack.shared:Found Directory: /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/input/\n",
      "INFO:flarestack.shared:Found Directory: /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/storage/\n",
      "INFO:flarestack.shared:Found Directory: /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/output/\n",
      "INFO:flarestack.shared:Found Directory: /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/cluster/\n",
      "INFO:flarestack.shared:Found Directory: /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/input/pull_corrections/\n",
      "INFO:flarestack.shared:Found Directory: /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/cluster/logs/\n",
      "INFO:flarestack.shared:Found Directory: /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/input/catalogues/\n",
      "INFO:flarestack.shared:Found Directory: /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/input/acceptance_functions/\n",
      "INFO:flarestack.shared:Found Directory: /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/input/energy_pdf_splines/\n",
      "INFO:flarestack.shared:Found Directory: /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/storage/pickles/\n",
      "INFO:flarestack.shared:Found Directory: /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/output/plots/\n",
      "INFO:flarestack.shared:Found Directory: /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/input/SoB_splines/\n",
      "INFO:flarestack.shared:Found Directory: /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/input/analysis/\n",
      "INFO:flarestack.shared:Found Directory: /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/output/plots/illustrations/\n",
      "INFO:flarestack.shared:Found Directory: /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/input/catalogues/transients/\n",
      "INFO:flarestack.shared:Found Directory: /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/input/bkg_splines/\n",
      "INFO:flarestack.shared:Found Directory: /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/output/dataset_plots/\n",
      "INFO:flarestack.shared:Found Directory: /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/output/unblinding_results/\n",
      "INFO:flarestack.shared:Found Directory: /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/output/limits/\n",
      "INFO:flarestack.shared:Found Directory: /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/input/pull_corrections/pulls/\n",
      "INFO:flarestack.shared:Found Directory: /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/input/pull_corrections/floors/\n",
      "INFO:flarestack.shared:Found Directory: /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/storage/cache/\n",
      "INFO:flarestack.shared:Found Directory: /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/storage/cache/catalogue_cache/\n",
      "INFO:flarestack.shared:Found Directory: /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/input/public_datasets/\n",
      "INFO:flarestack.shared:Found Directory: /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/input/energy_proxy_weighting/\n",
      "INFO:flarestack.shared:Found Directory: /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/output/dataset_plots/effective_area_plots/\n",
      "INFO:flarestack.shared:Found Directory: /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/input/median_angular_resolution/\n",
      "INFO:flarestack.shared:Found Directory: /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/output/dataset_plots/angular_resolution_plots/\n",
      "INFO:flarestack.shared:Found Directory: /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/output/dataset_plots/energy_proxy_map/\n",
      "INFO:flarestack.shared:Found Directory: /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/input/sim_datasets/\n",
      "INFO:numexpr.utils:Note: NumExpr detected 10 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "INFO:flarestack.data.icecube.ic_season:Loading datasets from /Users/thomasahrens/Desktop/IceCube/flarestack/datasets (local)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running at None, data directory is /Users/thomasahrens/Desktop/IceCube/flarestack/datasets\n"
     ]
    }
   ],
   "source": [
    "from flarestack.shared import host_server\n",
    "from flarestack.data.icecube.ic_season import icecube_dataset_dir\n",
    "print(f'Running at {host_server}, data directory is {icecube_dataset_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scratch directory is /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/\n"
     ]
    }
   ],
   "source": [
    "from flarestack.shared import fs_scratch_dir\n",
    "print(f'Scratch directory is {fs_scratch_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using Flarestack Classes\n",
    "\n",
    "Classes used in $\\texttt{flarestack}$'s core functionality (e.g. `flarestack.core.energy_pdf.EnergyPDF`, `flarestack.core.minimisation.MinimisationHandler`, etc) have a class attribute `<class>.subclasses`.  \n",
    "This is a dictionary with the structure `{<subclass name>: <subclass>}`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fixed_weights': flarestack.core.minimisation.FixedWeightMinimisationHandler,\n",
       " 'large_catalogue': flarestack.core.minimisation.LargeCatalogueMinimisationHandler,\n",
       " 'fit_weights': flarestack.core.minimisation.FitWeightMinimisationHandler,\n",
       " 'flare': flarestack.core.minimisation.FlareMinimisationHandler}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flarestack.core.minimisation import MinimisationHandler\n",
    "MinimisationHandler.subclasses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For analyses we only have to pass a dictionary of the subclass names and corresponding parameters.  \n",
    "To execute use `flarestack.cluster.submitter.Submitter`. This always works locally. For using the cluster, again, if you are running at DESY or WIPAC, you do not have to worry. We got you covered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'local': flarestack.cluster.submitter.LocalSubmitter,\n",
       " 'DESY': flarestack.cluster.submitter.DESYSubmitter,\n",
       " 'WIPAC': flarestack.cluster.submitter.WIPACSubmitter}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flarestack.cluster.submitter import Submitter\n",
    "Submitter.submitter_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example: Point Source Sensitivity ##\n",
    "\n",
    "Let's try to calculate the 10-year point source sensitivity for one declination.  \n",
    "First we have to specify a name for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flarestack.shared import plot_output_dir, name_pickle_output_dir\n",
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_types = ('fixed', 'fit', 'mcmc', 'hmc')\n",
    "\n",
    "def name_func(n_sources, gamma, min_type, etc=None):\n",
    "    \"\"\"Creates name for output analysis files.\n",
    "    \n",
    "    :param n_sources: Number of sources in catalog\n",
    "    :type n_sources: int\n",
    "    \n",
    "    :param gamma: Spectral index\n",
    "    :type n_sources: float\n",
    "    \n",
    "    :param min_type: Minimization method (see MinimisationHandler.subclasses)\n",
    "    :type n_sources: str\n",
    "    \n",
    "    :param etc: Additional simulation/run info\n",
    "    :type n_sources: str\n",
    "    \n",
    "    :return path: Path used as name\n",
    "    :rtype path: str\n",
    "    \"\"\"\n",
    "    run_no = 1\n",
    "    \n",
    "    if min_type not in min_types:\n",
    "        raise ValueError(f'Provide valid minimizer: {min_types}')\n",
    "        \n",
    "    path = f'analyses/{n_sources}source_gamma{gamma}_{min_type}'\n",
    "    \n",
    "    if etc is not None:\n",
    "        path += f'_{etc:s}'\n",
    "        \n",
    "    path += f'_run{run_no}'\n",
    "    \n",
    "    path_exist = os.path.exists(plot_output_dir(path)) or os.path.exists(name_pickle_output_dir(path))\n",
    "    \n",
    "    if path_exist:\n",
    "        # Automatically covers cases where run_no == (n_sources or gamma)\n",
    "        glob_path = path.split(f'_run{run_no}')[0]\n",
    "        # Get all runs with same path\n",
    "        previous_runs = glob(f'{name_pickle_output_dir(glob_path)}*')\n",
    "        print(path, previous_runs)\n",
    "        print(glob_path)\n",
    "        # Get run numbers for previous runs, convert strings to ints\n",
    "        run_nums = [int(i.split('_run')[1]) for i in previous_runs]\n",
    "        # Sort run numbers\n",
    "        run_nums.sort()\n",
    "        # Get last run number, increase index by 1\n",
    "        run_no = run_nums[-1] + 1\n",
    "        path = f'{glob_path}_run{run_no}'\n",
    "    else:\n",
    "        # Path DNE, unchanged path (run 1)\n",
    "        pass\n",
    "    \n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'analyses/5source_gamma2.0_fit_sumscale_trials100_vanilla_run1'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = name_func(n_sources=5, gamma=2.0, min_type='fit', etc='sumscale_trials100_vanilla')\n",
    "name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input directory (with the analysis dictionaries), the output directory (plots, p-values, etc) and the cache directory (saved trials, etc) will be created accordingly.   \n",
    "For example our plot output directory will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/output/plots/analyses/5source_gamma2.0_fit_sumscale_trials100_vanilla_run1',\n",
       " '/Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/storage/pickles/analyses/5source_gamma2.0_fit_sumscale_trials100_vanilla_run1')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flarestack.shared import plot_output_dir, name_pickle_output_dir\n",
    "plot_output_dir(name), name_pickle_output_dir(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many dataset implementations are available in `flarestack.data`. We will use the PS Tracks v3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flarestack.data.icecube import ps_v003_p02\n",
    "from flarestack.data.public import icecube_ps_3_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to inject a steady neutrino signal with a power law spectrum with $\\gamma=2.5$. For other Energy or Time PDFs check `flarestack.core.energy_pdf` and `flarestack.core.time_pdf`.   \\\n",
    "This is as straight forward as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "injection_energy = {\n",
    "    \"energy_pdf_name\": \"power_law\",\n",
    "    \"gamma\": 2.0\n",
    "}\n",
    "\n",
    "injection_time = {\n",
    "    \"time_pdf_name\": \"steady\"\n",
    "}\n",
    "\n",
    "inj_kwargs = {\n",
    "    \"injection_energy_pdf\": injection_energy,\n",
    "    \"injection_sig_time_pdf\": injection_time\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are looking for a steady signal with a power law spectrum. \n",
    "We assume the background to be constant in time.  \n",
    "We want to use the \"standard\" point source likelihood. More likelihood implementations in `flarestack.core.llh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llh_time = {\n",
    "    \"time_pdf_name\": \"steady\"\n",
    "}\n",
    "\n",
    "llh_energy = {\n",
    "    \"energy_pdf_name\": \"power_law\",\n",
    "}\n",
    "\n",
    "llh_time_bkg = {\n",
    "    \"time_pdf_name\": \"steady\"\n",
    "}\n",
    "\n",
    "llh_kwargs = {\n",
    "    \"llh_name\": \"standard\",\n",
    "    \"llh_energy_pdf\": llh_energy,\n",
    "    \"llh_sig_time_pdf\": llh_time,\n",
    "    \"llh_bkg_time_pdf\": llh_time_bkg\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a source catalogue. This catalogue will be a numpy array stored as a `.npy` file and we only pass the filename.   \n",
    "For point sources the is a uitility function to generate dummy sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your catalogue is located at /Users/thomasahrens/Desktop/IceCube/sn-search/catalog/test_catalogue_5.npy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from flarestack.utils.prepare_catalogue import ps_catalogue_name\n",
    "import numpy as np\n",
    "\n",
    "sindec = 0.5\n",
    "catalogue_path = \"/Users/thomasahrens/Desktop/IceCube/sn-search/catalog/test_catalogue_5.npy\"\n",
    "print(f'your catalogue is located at {catalogue_path}')\n",
    "catalogue = np.load(catalogue_path)\n",
    "type(catalogue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make a guess for our sensitivity.   \n",
    "Note: $\\texttt{flarestack}$ is using its own scale factor $k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999999999.9999999"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flarestack.shared import flux_to_k\n",
    "flux_to_k(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we know where the sensitivity should be. Because the analysis has been done before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.24859081, 24.92108471, 27.77154076,  1.27010269,  4.2832197 ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.basicConfig(level='ERROR')\n",
    "from flarestack.icecube_utils.reference_sensitivity import reference_sensitivity\n",
    "scale = flux_to_k(reference_sensitivity(np.sin(catalogue['dec_rad']))) * 3\n",
    "scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just have to put all the info into one dictionary to pass to the `MinimisationHanddler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mh_dict = {\n",
    "    \"name\": name,                               # unique name for the analysis\n",
    "    \"mh_name\": \"fixed_weights\",                 # name of the MinimisationHandler subcalss\n",
    "    \"dataset\": icecube_ps_3_year.get_seasons(),                     # the neutrino dataset\n",
    "    \"catalogue\": catalogue_path,                # path to the .npy catalogue file\n",
    "    \"inj_dict\": inj_kwargs,                     # info for the Injector\n",
    "    \"llh_dict\": llh_kwargs,                     # info for the LLH\n",
    "    \"scale\": np.sum(scale),                     # a guess for the sensitivity scale\n",
    "    \"n_trials\": 100,                            # number of trials to run (background trials will be run ten times this number!)\n",
    "    \"n_steps\": 10,                              # number of steps when injecting signal\n",
    "    \"allow_extrapolated_sensitivity\": True      # allow extrapolation in the sensitivity calculation (here we do because we only run very few trials)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute the analysis we defined above we create a submitter instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:flarestack.cluster.submitter:No submitter implemented for host server None! Using LocalSubmitter but you wont't be able to use cluster operations!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Submitter for analyses/5source_gamma2.0_fit_sumscale_trials100_vanilla_run1 -----\n",
      "not using cluster \n",
      "using 7 CPUs locally\n",
      "job-id: None \n",
      "no scale estimation \n",
      "\n"
     ]
    }
   ],
   "source": [
    "submitter = Submitter.get_submitter(\n",
    "    mh_dict=mh_dict,                         # the analysis info\n",
    "    use_cluster=False,                       # run it on the cluster if True\n",
    "    n_cpu=7,                                 # number of LOCAL CPUs to use, NOTE: the number of cluster CPUs has to be specified in the cluster_kwargs!\n",
    "    do_sensitivity_scale_estimation=False,   # make a guess of the sensitivity scale, for options check flarestack.cluster.submitter\n",
    "    remove_old_results=True,                 # if you are running the analysis again and something changed, maybe you want to remove old trials?\n",
    "#   **cluster_kwargs                         # keyword arguments used when running the cluster, This depends on the cluster obviously\n",
    ")\n",
    "\n",
    "print(submitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Energise ......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:flarestack.cluster.submitter:Can not remove /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/storage/pickles/analyses/5source_gamma2.0_fit_sumscale_trials100_vanilla_run1! It is not a directory!\n",
      "WARNING:flarestack.cluster.submitter:Can not remove /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/storage/pickles/injection_values/analyses/5source_gamma2.0_fit_sumscale_trials100_vanilla_run1! It is not a directory!\n",
      "INFO:flarestack.core.minimisation:Using 'standard' LLH class\n",
      "INFO:flarestack.core.injector:Initialising Injector for IC79-2010\n",
      "INFO:flarestack.core.injector:Initialising Injector for IC86-2011\n",
      "INFO:flarestack.core.injector:Initialising Injector for IC86-2012\n",
      "INFO:flarestack.core.minimisation:Using 'standard' LLH class\n",
      "INFO:flarestack.core.minimisation:Using 'standard' LLH class\n",
      "INFO:flarestack.core.minimisation:Using 'standard' LLH class\n",
      "INFO:flarestack.core.minimisation:Using 'standard' LLH class\n",
      "INFO:flarestack.core.minimisation:Using 'standard' LLH class\n",
      "INFO:flarestack.core.multiprocess_wrapper:Added 1900 trials to queue. Now processing.\n",
      "INFO:flarestack.core.minimisation:Using 'standard' LLH class\n",
      "INFO:flarestack.core.multiprocess_wrapper:1900 tasks remaining.\n",
      "INFO:flarestack.core.minimisation:Using 'standard' LLH class\n",
      "/Users/thomasahrens/Desktop/IceCube/flarestack/flarestack/core/llh.py:912: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  kwargs[\"SoB_spacetime_cache\"] = np.array(SoB_spacetime)\n",
      "/Users/thomasahrens/Desktop/IceCube/flarestack/flarestack/core/llh.py:912: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  kwargs[\"SoB_spacetime_cache\"] = np.array(SoB_spacetime)\n",
      "/Users/thomasahrens/Desktop/IceCube/flarestack/flarestack/core/llh.py:912: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  kwargs[\"SoB_spacetime_cache\"] = np.array(SoB_spacetime)\n",
      "/Users/thomasahrens/Desktop/IceCube/flarestack/flarestack/core/llh.py:912: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  kwargs[\"SoB_spacetime_cache\"] = np.array(SoB_spacetime)\n",
      "/Users/thomasahrens/Desktop/IceCube/flarestack/flarestack/core/llh.py:912: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  kwargs[\"SoB_spacetime_cache\"] = np.array(SoB_spacetime)\n",
      "/Users/thomasahrens/Desktop/IceCube/flarestack/flarestack/core/llh.py:912: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  kwargs[\"SoB_spacetime_cache\"] = np.array(SoB_spacetime)\n",
      "/Users/thomasahrens/Desktop/IceCube/flarestack/flarestack/core/llh.py:912: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  kwargs[\"SoB_spacetime_cache\"] = np.array(SoB_spacetime)\n",
      "INFO:flarestack.core.multiprocess_wrapper:972 tasks remaining.\n",
      "INFO:flarestack.core.multiprocess_wrapper:893 tasks remaining.\n",
      "INFO:flarestack.core.multiprocess_wrapper:867 tasks remaining.\n",
      "INFO:flarestack.core.multiprocess_wrapper:655 tasks remaining.\n",
      "INFO:flarestack.core.multiprocess_wrapper:156 tasks remaining.\n",
      "INFO:flarestack.core.multiprocess_wrapper:Finished processing 1900 tasks.\n"
     ]
    }
   ],
   "source": [
    "submitter.analyse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the results we use the `ResultsHandler`. This will also create some plots like the sensitivity fit, bias plots, etc. in the plot directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:flarestack.core.results:Saving bias plot to /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/output/plots/analyses/5source_gamma2.0_fit_sumscale_trials100_vanilla_run1/bias_n_s.pdf\n",
      "INFO:flarestack.core.results:Saving bias plot to /Users/thomasahrens/Desktop/IceCube/flarestack/scratch/flarestack__data/output/plots/analyses/5source_gamma2.0_fit_sumscale_trials100_vanilla_run1/bias_gamma.pdf\n",
      "INFO:flarestack.core.results:Fraction of overfluctuations is 0.23 above 0.00 (N_trials=1000) (Scale=0)\n",
      "INFO:flarestack.core.results:Fraction of overfluctuations is 0.94 above 0.00 (N_trials=100) (Scale=7.166)\n",
      "INFO:flarestack.core.results:Fraction of overfluctuations is 0.99 above 0.00 (N_trials=100) (Scale=14.33)\n",
      "INFO:flarestack.core.results:Fraction of overfluctuations is 1.00 above 0.00 (N_trials=100) (Scale=21.5)\n",
      "INFO:flarestack.core.results:Fraction of overfluctuations is 1.00 above 0.00 (N_trials=100) (Scale=28.66)\n",
      "INFO:flarestack.core.results:Fraction of overfluctuations is 1.00 above 0.00 (N_trials=100) (Scale=35.83)\n",
      "INFO:flarestack.core.results:Fraction of overfluctuations is 1.00 above 0.00 (N_trials=100) (Scale=43)\n",
      "INFO:flarestack.core.results:Fraction of overfluctuations is 1.00 above 0.00 (N_trials=100) (Scale=50.16)\n",
      "INFO:flarestack.core.results:Fraction of overfluctuations is 1.00 above 0.00 (N_trials=100) (Scale=57.33)\n",
      "INFO:flarestack.core.results:Fraction of overfluctuations is 1.00 above 0.00 (N_trials=100) (Scale=64.49)\n",
      "INFO:flarestack.core.results:Sensitivity is 5.76e-09\n",
      "INFO:flarestack.core.results:Fraction of overfluctuations is 0.00 above 24.30 (N_trials=1000) (Scale=0)\n",
      "INFO:flarestack.core.results:Fraction of overfluctuations is 0.00 above 25 (N_trials=1000) (Scale=0)\n",
      "INFO:flarestack.core.results:Fraction of overfluctuations is 0.07 above 24.30 (N_trials=100) (Scale=7.166)\n",
      "INFO:flarestack.core.results:Fraction of overfluctuations is 0.07 above 25 (N_trials=100) (Scale=7.166)\n",
      "INFO:flarestack.core.results:Fraction of overfluctuations is 0.57 above 24.30 (N_trials=100) (Scale=14.33)\n",
      "INFO:flarestack.core.results:Fraction of overfluctuations is 0.56 above 25 (N_trials=100) (Scale=14.33)\n",
      "INFO:flarestack.core.results:Fraction of overfluctuations is 0.91 above 24.30 (N_trials=100) (Scale=21.5)\n",
      "INFO:flarestack.core.results:Fraction of overfluctuations is 0.90 above 25 (N_trials=100) (Scale=21.5)\n",
      "INFO:flarestack.core.results:Fraction of overfluctuations is 0.97 above 24.30 (N_trials=100) (Scale=28.66)\n",
      "INFO:flarestack.core.results:Fraction of overfluctuations is 0.96 above 25 (N_trials=100) (Scale=28.66)\n",
      "INFO:flarestack.core.results:Fraction of overfluctuations is 1.00 above 24.30 (N_trials=100) (Scale=35.83)\n",
      "INFO:flarestack.core.results:Fraction of overfluctuations is 1.00 above 25 (N_trials=100) (Scale=35.83)\n",
      "INFO:flarestack.core.results:Fraction of overfluctuations is 1.00 above 24.30 (N_trials=100) (Scale=43)\n",
      "INFO:flarestack.core.results:Fraction of overfluctuations is 1.00 above 25 (N_trials=100) (Scale=43)\n",
      "INFO:flarestack.core.results:Fraction of overfluctuations is 1.00 above 24.30 (N_trials=100) (Scale=50.16)\n",
      "INFO:flarestack.core.results:Fraction of overfluctuations is 1.00 above 25 (N_trials=100) (Scale=50.16)\n",
      "INFO:flarestack.core.results:Fraction of overfluctuations is 1.00 above 24.30 (N_trials=100) (Scale=57.33)\n",
      "INFO:flarestack.core.results:Fraction of overfluctuations is 1.00 above 25 (N_trials=100) (Scale=57.33)\n",
      "INFO:flarestack.core.results:Fraction of overfluctuations is 1.00 above 24.30 (N_trials=100) (Scale=64.49)\n",
      "INFO:flarestack.core.results:Fraction of overfluctuations is 1.00 above 25 (N_trials=100) (Scale=64.49)\n",
      "INFO:flarestack.core.results:Discovery Potential is 1.34e-08\n",
      "INFO:flarestack.core.results:Discovery Potential (TS=25) is 1.34e-08\n"
     ]
    }
   ],
   "source": [
    "from flarestack.core.results import ResultsHandler\n",
    "results_handler = ResultsHandler(submitter.mh_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity flux: 5.76e-09 +1.029246364787624e-08  -2.249846112062059e-09\n",
      "reference: 5.770894709671131e-10\n",
      "sensitivity n_s: 3.51e+00 +6.272874919985622  -1.3711977747031776\n"
     ]
    }
   ],
   "source": [
    "print(fr'sensitivity flux: {results_handler.sensitivity:.2e} +{results_handler.sensitivity_err[1]}  -{results_handler.sensitivity_err[0]}')\n",
    "print(f'reference: {reference_sensitivity(sindec)[0]}')\n",
    "print(fr'sensitivity n_s: {results_handler.sensitivity * results_handler.flux_to_ns:.2e} +{results_handler.sensitivity_err[1] * results_handler.flux_to_ns}  -{results_handler.sensitivity_err[0] * results_handler.flux_to_ns}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Example: Upper Limits\n",
    "\n",
    "There are some really useful functions in `flarestack.cosmo`!  \n",
    "\n",
    "Take the implementations of the IceCube diffuse flux measurements for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flarestack.cosmo.icecube_diffuse_flux import contours, get_diffuse_flux_contour\n",
    "contours.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "for fit in contours.keys():\n",
    "\n",
    "    best_fit, upper_butterfly, lower_butterfly, e_range = get_diffuse_flux_contour(fit)\n",
    "    plt.plot(e_range, best_fit(e_range) * e_range**2, label=fit)\n",
    "    plt.fill_between(e_range, upper_butterfly(e_range)* e_range**2, lower_butterfly(e_range)* e_range**2, alpha=0.3)\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(r\"$E_{\\nu}$\")\n",
    "plt.ylabel(r\"$E_{\\nu}^{2} \\frac{dN}{dE}$\")\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.4), ncol=2, fancybox=True, shadow=True, fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested in transients you also are at the right place!  \n",
    "With your favourte transients population's rate, the flux normaisation at 1 GeV and the corresponding spectral index you can easily get the rate in a redshift shell, the neutrino flux per source at a certain redshift, the neutrino flux per redshift and the cumulatice neutrino flux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flarestack.cosmo.neutrino_cosmology import define_cosmology_functions\n",
    "from flarestack.cosmo.rates import get_rate\n",
    "from astropy import units as u\n",
    "\n",
    "frb_rate = get_rate('FRB')\n",
    "frb_dummy_flux = 2e+46 / u.GeV\n",
    "frb_example_gamma = 2\n",
    "\n",
    "rate_per_z, nu_flux_per_z, nu_flux_per_source, cumulative_nu_flux = define_cosmology_functions(frb_rate, frb_dummy_flux, frb_example_gamma)\n",
    "\n",
    "redshift = np.linspace(0.1, 4, 1000)\n",
    "\n",
    "fig, axs = plt.subplots(4, sharex='all', figsize=[5, 12])\n",
    "axs[0].plot(redshift, rate_per_z(redshift).to('yr-1').value)\n",
    "axs[0].set_ylabel('rate [yr$^{-1}$]')\n",
    "\n",
    "axs[1].plot(redshift, nu_flux_per_z(redshift).to('GeV-1 cm-2 s-1 sr-1').value)\n",
    "axs[1].set_ylabel(r'$\\nu$ flux per redshift [GeV$^{-1}$ cm$^{-2}$]')\n",
    "\n",
    "axs[2].plot(redshift, nu_flux_per_source(redshift).to('GeV-1 cm-2').value)\n",
    "axs[2].set_ylabel(r'$\\nu$ flux per source [GeV$^{-1}$ cm$^{-2}$]')\n",
    "\n",
    "axs[3].plot(redshift[1:-1], [i.to('1 / (cm2 GeV s sr)').value for i in cumulative_nu_flux(redshift)])\n",
    "axs[3].set_ylabel(r'cumulative $\\nu$ flux [GeV$^{-1}$ cm$^{-2}$]')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: The result of `cumulative_nu_flux()` is already your result for the contribution of the popultion to the diffuse flux!  \n",
    "\n",
    "All the above is packed into one convenience function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flarestack.cosmo.neutrino_cosmology import calculate_transient_cosmology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this to get some actual super interesting, timely results and revisit the FRB  asscociated with the galactic Magnetar SGR 1935+2154 (https://arxiv.org/abs/2005.10828).  \n",
    "IceCube performed a search for neutrinos and found upper limits:\n",
    "```\n",
    "IceCube Limit is E^2 dN/dE = 5.2 × 10−2 GeV cm^-2 @ 1 GeV \n",
    "```\n",
    "(http://www.astronomerstelegram.org/?read=13689)  \\\n",
    "The Magnetar is 16 kpc away (conservative). Let's assume a spectrum with $\\gamma=2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flarestack.core.energy_pdf import EnergyPDF\n",
    "\n",
    "dist = 16 * u.kpc\n",
    "atel_flux_norm_lim = 5.2 * 10**-2. * (u. GeV / u.cm**2) / u.GeV**2.\n",
    "\n",
    "e_pdf_dict = {\n",
    "    \"energy_pdf_name\": \"power_law\",\n",
    "    \"gamma\": 2.0,\n",
    "    \"e_min_gev\": 10.**3,\n",
    "    \"e_max_gev\": 10.**6,\n",
    "    \"nu_flux_at_1_gev\": atel_flux_norm_lim * 4 * np.pi * dist**2.\n",
    "}\n",
    "\n",
    "epdf = EnergyPDF.create(e_pdf_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these information it is now super straight forward to get the upper limits from a population of FRB's, that share SGR 1935+2154's properties, assuming they are all standard candels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = \"joint_15\"\n",
    "integrated_nu_flux_1_gev = calculate_transient_cosmology(e_pdf_dict, frb_rate, \"frb_limit\", zmax=8.0, diffuse_fit=fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot our very interesting findings so we can publish in a prestegious journal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fit, upper_butterfly, lower_butterfly, e_range = get_diffuse_flux_contour(fit=fit)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(e_range, best_fit(e_range) * e_range**2, label=\"IceCube Diffuse Flux\")\n",
    "plt.fill_between(e_range, upper_butterfly(e_range)* e_range**2, lower_butterfly(e_range)* e_range**2, alpha=0.3)\n",
    "x = [epdf.e_min, np.exp(0.5*(np.log(epdf.e_min) + np.log(epdf.e_max))), epdf.e_max]\n",
    "y = np.array([integrated_nu_flux_1_gev.value for _ in range(3)]) \n",
    "plt.errorbar(x, y, yerr=0.25*y, uplims=True, label='limit')\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(r\"$E_{\\nu} [GeV] $\")\n",
    "plt.ylabel(r\"$E_{\\nu}^{2} \\frac{dN}{dE}$ [GeV cm$^{-2}$ s$^{-1}$ sr$^{-1}$]\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
